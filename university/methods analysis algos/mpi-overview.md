# Основы работы с Microsoft MPI

[<- Назад (Методы построения и анализа алгоритмов)](https://github.com/boorlakov/zettelkasten/blob/main/university/methods%20analysis%20algos/README.md)

## About

Одним из наиболее распространенных технологий программирования в системах с распределенной памятью является использование интерфейса передачи сообщений **`Message Passing Interface`** – **`MPI`**.

В `MPI` используется следующая модель параллельного программирования: программа представляется в виде множества процессов, параллельно исполняемых на различных вычислительных узлах. При этом программы процессов написаны с использованием последовательных языков программирования, таких как `C`/`C++` и Фортран. Для управления этими параллельными процессами используется набор команд для передачи сообщений.

Стандарт `MPI` непрерывно развивается, со временем в него включаются новые возможности. Существуют различные его реализации: `MPICH`, `Open MPI`, `Microsoft MPI`, `Intel MPI` и др. Мы рассмотрим использование `Microsoft MPI` для языка `С`.

Для параллельной программы, работающей в вычислительном кластере или в локальной сети, необходим механизм запуска процессов на удаленных компьютерах. В `Windows` возможность непосредственного простого запуска процессов в памяти удаленной машины ограничена средствами безопасности: правами пользователя, сетевым экраном и т.д. MPI дает такие возможности, а также предоставляет интерфейс обмена данными между процессами, запущенными на удаленных машинах.

Продемонстрируем принцип работы `MPI` на следующем примере. Пусть нам необходимо запустить программу `program.exe` на **десяти** компьютерах с именами `machine1`, `machine2`, `machine3`, `...`, `machine10`. Эти компьютеры должны быть объединены в локальную сеть. Программа `program.exe` написана на последовательном языке программирования, например, `C`/`C++`, предварительно откомпилирована и скопирована на все десять компьютеров в папку с одинаковым именем (или же находится на сетевом диске, к которому имеют доступ все компьютеры).

Менеджер процессов `smpd.exe`, установленный на каждой из удаленных машин, постоянно прослушивает **определенный порт** (пусть для определенности это будет порт **`1234`**, его номер можно задать в настройках, по умолчанию он должен быть одинаковым для всех используемых компьютеров). Связь между менеджерами процессов, установленными на разных машинах, осуществляется средствами библиотеки `Winsock` по протоколу `TCP`/`IP`. Как только по сети приходит нужная команда, менеджер процессов `smpd.exe`, установленный на удаленной машине, запускает копию `program.exe` на этой удаленной машине.

Принцип работы механизма удаленного запуска процессов в `MPICH` схематично изображен на следующем рисунке.

Для связи нашей программы `program.exe` и менеджера процессов smpd.exe служит еще одна утилита с именем файла `mpiexec.exe`. С ее помощью осуществляется запуск `MPI`-программ. Приведем основные опции команды `mpiexec.exe`.

- запустить процессы в количестве **`x`** штук

  ```bat
  -n x
  ```

- запустить процессы на компьютерах, список которых содержится в файле `filename` (это могут быть имена компьютеров в сети или `IP`-адреса)

  ```bat
  -machinefile filename
  ```

- запустить процесс на компьютере с именем **`hostname`**

  ```bat
  -host hostname
  ```

Например, следующая команда запустит 10 копий программы `program.exe` на компьютерах, `IP`-адреса которых перечислены в файле `file`:

```bat
mpiexec.exe -n 10 -machinefile file program.exe
```

В итоге, для обеспечения запуска параллельной `MPICH`-программы в среде `Windows` необходимо для исполняемых файлов `smpd.exe`, `mpiexec.exe` и нашей программы `program.exe` установить права администратора, разрешить их запуск через сетевой экран (на каждой из установленных машин).

**Желательно** запускать `MPI`-программы на всех компьютерах под **одинаковой учетной записью** (и с одинаковым паролем).

Также допускается запускать по нескольку `MPI`-процессов на одном компьютере. При этом следует учитывать, что если число `MPI`-процессов **превышает** число процессоров/ядер, то, разумеется, будет наблюдаться **падение производительности**.
