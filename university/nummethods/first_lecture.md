# Лекция 1

## Литература

1. Численные методы. Вержбицкий (ЧМ анализа, ЧМ линейной алгебры и объединение книжек)

## Содержание

Что такое итерационные методы — методы, в которых решение идет последовательно, число шагов = точность, каждый следующий элемент после выражается через предыдущий:
Решая Ax = b

Не хочется матрицу преобразовать, выглядит это так

X_k+1 = B x_k + c

Самый простой метод

### Метод Якоби

Считаем, что A = L + D + U, а считаем, что
B = - D^-1 (L + U), а c = D^-1 b

Решая

[2 1] [1 2]  x = [3][3], где B = [0 -1/2] [-1/2 0]

c = x_k+1 = (3/2),   x_k+2 = (3/4)      x_k+3 = (9/8)
            (3/2)            (3/4)              (9/8),

Получаем ответ только в пределе.

У всех методов итерационных есть проблема, что все теоремы не строгие (например, для Якоби условие сходимости только при ||B|| < 1)

Вспомнили про норму матрицы

### Метод Гаусса-Зейделя

```LaTeX
\begin{bmatrix} 
a & b & c \\
c & d & d\\
e & f & g \\
\end{bmatrix}
```

B = -(L + D)^(-1) U
c =  (L + D)^(-1) b

((L + D)^(-1) (U x_k))

Это модификация метода Якоби
Как определить класс матриц, где выражение сходится? (Как минимум для всех тех же матриц диагонального преобладания)

Какие еще могут быть улучшения?

№1 метод последовательной верной релаксации
Хочу, чтобы уравнение первой строки выполнилось
(X_k+1 = B x_k + c)
Потом говорим, что теперь удовлетворяем, портя предыдущую
. . .
Дошли до эн, вернулись обратно
На самом деле это метод Зейделя

Что делать если матрица Б явно не написана?
Указание на то, что это линейная операция

Есть вектор x_k+1 - x_k, давайте сделаем x_k+1* = x_k + w(x_k+1 - x_k), где
0 < w < 2
Но есть нюанс, что только если вы знаете собственные числа матрицы, то тогда метод будет сходится максимально быстро (бесполезно ирл), потому что собственные числа искать гораздо дольше.

### Как улучшить итерационные методы?

Например, посмотреть на несколько шагов назад!

Для положительно определенных хороших матриц идеально подойдет соус Терияки метод сопряженных градиентов
Он помнит все вектора назад. И на самом деле он не очень итерационный. За эн шагов вы получите гарантированную сходимость

### Некоторые нюансы

1. Не пугайтесь разным формулам. Это просто другая нотация. Они все эквиваленты. Можете проверить программно

2. В каком-то смысл он не совсем итерационный

3. На z_k нужно умножить один раз

4. Так же как и для метода Зейделя на 1 итерации одно умножение (самая дорогая операция)

### Несколько слов по поводу сходимости

1. Когда норма матрицы Б больше 1, то точно не сходится

2. Накапливается погрешность — обусловленность матрицы (разнопорядковые числа на диагонали)

3. Для измерения обусловленности вводят еще одно число
Cond(A) Число Тодда = lambda_max / lambda_min
4. Какое число считать большим — это сложный вопрос
    - Обычно если Тодд > 10k, то уже плохо

5. Чем Тодд больше, тем сходится хуже и быстрее копится ошибка

### Еще улучшения итерационных методов

А давайте поулучшаем число обусловленности! За дополнительные действия мы получим матрицу А чуть получше и за то все итерационные методы начнут замечательно сходиться

Как? А вот как:

1. Допустим матрица А квадратная неразреженная и смогли построить LU-разложение и научились решать систему L^-1 A (U^-1 y) = L^-1 b

2. Все итерационные методы сойдутся за 1 операцию

3. А где собака-то зарыта? А нафига козе баян, если уже все сделали

4. Как можно брать LU? Странно, но возьмем прям L U из A = L + D + U

Есть еще метод: неполное разложение Холецкого
Заводим L U с этим же портретом с формулами обычного разложения Холецкого.

Курите мануалы!

В последнее время используют итерационные методы + ???
